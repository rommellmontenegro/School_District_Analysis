{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aeaf3e8-850d-4cff-b3ee-ffb6a20c236c",
   "metadata": {},
   "source": [
    "# ***Google Drive Bootup***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d89a9adb-59cc-4416-bd10-17ddf904652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from httplib2 import Http\n",
    "from httplib2 import Http\n",
    "from apiclient.http import MediaFileUpload,MediaIoBaseDownload\n",
    "import io\n",
    "\n",
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive','https://www.googleapis.com/auth/drive.file', 'https://www.googleapis.com/auth/drive.metadata']\n",
    "\n",
    "creds = None\n",
    "  # The file token.json stores the user's access and refresh tokens, and is\n",
    "  # created automatically when the authorization flow completes for the first\n",
    "  # time.\n",
    "if os.path.exists(\"token.json\"):\n",
    "    creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
    "  # If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "      creds.refresh(Request())\n",
    "    else:\n",
    "      flow = InstalledAppFlow.from_client_secrets_file(\n",
    "          \"client_secret_833249459653-01pcjla0c9re30fgr2dfipcgg5lgq6j0.apps.googleusercontent.com.json\", SCOPES\n",
    "      )\n",
    "      creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open(\"token.json\", \"w\") as token:\n",
    "      token.write(creds.to_json())\n",
    "    \n",
    "# Call the Drive v3 API\n",
    "drive = build(\"drive\", \"v3\", credentials=creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aef6a7-6335-4253-a325-b8765c23be37",
   "metadata": {},
   "source": [
    "# ***Downloads ALL Folders***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59deab1-c279-4842-ab4e-ffeef5f34afe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ***Verifies and Reports Files and Folders***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ae028f-2ea4-449c-bef7-bb5a09cac916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5k/m2kjbr8969j9g5j1fq8vnxkm0000gn/T/ipykernel_52219/4097022071.py:138: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['File Type'].replace('docx', '.docx', inplace=True)\n",
      "/var/folders/5k/m2kjbr8969j9g5j1fq8vnxkm0000gn/T/ipykernel_52219/4097022071.py:139: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['File Type'].replace('.MOV', '.mov', inplace=True)\n",
      "/var/folders/5k/m2kjbr8969j9g5j1fq8vnxkm0000gn/T/ipykernel_52219/4097022071.py:140: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['File Type'].replace('xlsx', '.xlsx', inplace=True)\n",
      "/var/folders/5k/m2kjbr8969j9g5j1fq8vnxkm0000gn/T/ipykernel_52219/4097022071.py:141: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['File Type'].replace('.JPG', '.jpg', inplace=True)\n",
      "/var/folders/5k/m2kjbr8969j9g5j1fq8vnxkm0000gn/T/ipykernel_52219/4097022071.py:142: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['File Type'].replace('HEIC', '.heic', inplace=True)\n",
      "/var/folders/5k/m2kjbr8969j9g5j1fq8vnxkm0000gn/T/ipykernel_52219/4097022071.py:143: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['File Type'].replace('PNG', '.png', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "FolderName = 'ALL'\n",
    "PC = 14\n",
    "\n",
    "query = \"mimeType = 'application/vnd.google-apps.folder'\"\n",
    "\n",
    "from time import mktime\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "global dffilecount, filename, foldersize, folderid, FolderName2\n",
    "\n",
    "currentDateTime = datetime.now()\n",
    "date = currentDateTime.date()\n",
    "year = date.strftime(\"%Y\") #except: 2022\n",
    "date.strftime(\"%Y\")\n",
    "\n",
    "d = datetime.now()\n",
    "only_date, only_time = d.date(), d.time()\n",
    "only_date = only_date.strftime('%Y.%m.%d')\n",
    "only_time = only_time.strftime('%H.%M')\n",
    "procTS = str(only_date) + \"_\" + str(only_time)\n",
    "dffilecount = 0\n",
    "FolderName2 = FolderName.replace(\"'\", \"\")\n",
    "\n",
    "if PC == 13:\n",
    "    Base = \"/Volumes/My Passport for Mac/Evidence/5 - Mobile Graphic & Video Media/\"\n",
    "    Local = \"/Users/rommell/Desktop/L - Civil Suit/RICO Album Tables/\"\n",
    "    External = \"/Volumes/My Passport for Mac/Evidence/5 - Media Documentation/\"\n",
    "\n",
    "if PC == 14:\n",
    "    Base = \"/Volumes/Files/2022 MacBook Pro/LARGE FOLDERS/Civil Suit & Evidence/Civil Suit*/5 - Mobile Graphic & Video Media/11.17.24 RICO at Hip/\"\n",
    "    Local = \"/Users/rommellmontenegro/Desktop/Civil Suit*/5 - Mobile Graphic & Video Media/RICO Album Tables/ADMIN-Working Files/GDrive Folder Lists/GDrive All Folders/\"\n",
    "    External = \"/Volumes/Files/2022 MacBook Pro/LARGE FOLDERS/Civil Suit & Evidence/Civil Suit*/5 - Mobile Graphic & Video Media/FILES TABLES/\"\n",
    "\n",
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "files = []\n",
    "\n",
    "creds = None\n",
    "  # The file token.json stores the user's access and refresh tokens, and is\n",
    "  # created automatically when the authorization flow completes for the first\n",
    "  # time.\n",
    "if os.path.exists(\"token.json\"):\n",
    "    creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
    "  # If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "      creds.refresh(Request())\n",
    "    else:\n",
    "      flow = InstalledAppFlow.from_client_secrets_file(\n",
    "          \"client_secret_833249459653-01pcjla0c9re30fgr2dfipcgg5lgq6j0.apps.googleusercontent.com.json\", SCOPES\n",
    "      )\n",
    "      creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open(\"token.json\", \"w\") as token:\n",
    "      token.write(creds.to_json())\n",
    "\n",
    "# Call the Drive v3 API\n",
    "drive = build(\"drive\", \"v3\", credentials=creds)\n",
    "\n",
    "# First, get the folder ID by querying by mimeType and name\n",
    "folderId = drive.files().list(q = query, pageSize=1000, fields=\"nextPageToken,files(id, name,webViewLink,size)\").execute()\n",
    "\n",
    "# this gives us a list of all folders with that name\n",
    "folderIdResult = folderId.get('files', [])\n",
    "# however, we know there is only 1 folder with that name, so we just get the id of the 1st item in the list\n",
    "id = folderIdResult[0].get('id')\n",
    "folderid = pd.Series(id)\n",
    "\n",
    "# Now, using the folder ID gotten above, we get all the files from that particular folder in a dictionary file of the resulting file id, name, and weblink FOR ONLY the first 1000 results.\n",
    "results = drive.files().list(q = \"'\" + id + \"' in parents\", pageSize=1000, fields=\"nextPageToken, files(id, name,webViewLink,size)\").execute()\n",
    "\n",
    "#Creates in a smaller dictionary file with only the data of file id, name, and weblink which can be made into a dataframe object.\n",
    "items = results.get('files', [])\n",
    "\n",
    "page_token = None\n",
    "\n",
    "\n",
    "while True:\n",
    "# Now, using the folder ID gotten above, we get all the files from that particular folder in a dictionary file of the resulting file id, name, and weblink results.\n",
    "  response = (\n",
    "      drive.files()\n",
    "      .list(\n",
    "          #q= \"'\" + id + \"' in parents\",\n",
    "          q=\"mimeType = 'application/vnd.google-apps.folder'\",\n",
    "          spaces=\"drive\",\n",
    "          fields=\"nextPageToken, files(id, name, webViewLink, size)\",\n",
    "          pageToken=page_token,\n",
    "      )\n",
    "      .execute()\n",
    "  )\n",
    "\n",
    "#Creates in a smaller dictionary file with only the data of file id, name, and weblink which can be made into a dataframe object.\n",
    "  for file in response.get(\"files\", []):\n",
    "    items.append(file)\n",
    "  files.extend(response.get(\"files\", []))\n",
    "\n",
    "  page_token = response.get(\"nextPageToken\", None)\n",
    "  if page_token is None:\n",
    "    break\n",
    "\n",
    "#Creates the dataframe of the results    \n",
    "df = pd.DataFrame(items)\n",
    "df = df.drop_duplicates()\n",
    "if 'name' not in df.columns:\n",
    "    df['name'] = np.nan\n",
    "    df = df[df.name != '.DS_Store']\n",
    "#    df.drop(df.loc[df['name']=='.DS_Store'].index, inplace=True)\n",
    "#    df.drop(df[df['name'] == '.DS_Store'].index, inplace=True)\n",
    "dffilecount = str(len(df))\n",
    "\n",
    "#df = df.sort_values(by='size')\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df['date-time processed'] = procTS\n",
    "if 'size' not in df.columns:\n",
    "    df['size'] = np.nan\n",
    "df['size'] = df['size'].fillna(0)\n",
    "df['size'] = df['size'].astype(int)\n",
    "foldersize = df['size'].sum()\n",
    "df['MB_size'] = df['size']/1000000\n",
    "\n",
    "\n",
    "df['File Type'] = np.where(df['size'] == 0, 'Folder', df['name'].str[-4:])\n",
    "\n",
    "df['File Type'].replace('docx', '.docx', inplace=True)\n",
    "df['File Type'].replace('.MOV', '.mov', inplace=True)\n",
    "df['File Type'].replace('xlsx', '.xlsx', inplace=True)\n",
    "df['File Type'].replace('.JPG', '.jpg', inplace=True)\n",
    "df['File Type'].replace('HEIC', '.heic', inplace=True)\n",
    "df['File Type'].replace('PNG', '.png', inplace=True)\n",
    "\n",
    "unique_names = df['File Type'].unique()\n",
    "\n",
    "column_order = ['name', 'id', 'webViewLink', 'File Type', 'date-time processed', 'MB_size']\n",
    "tabname = FolderName2[:30]\n",
    "\n",
    "suffix = \"_GDrive_Files.xlsx\"\n",
    "filename = only_date + \"_\" + only_time + \"_\" + FolderName2 + \"_(\" + dffilecount + \")\" + suffix\n",
    "HD_Dest = Local + filename\n",
    "\n",
    "tab1 = df[:1001]\n",
    "tab2 = df[1001:2001]\n",
    "tab3 = df[2001:3001]\n",
    "tab4 = df[3001:4001]\n",
    "tab5 = df[4001:5001]\n",
    "tab6 = df[5001:6001]\n",
    "tab7 = df[6001:7001]\n",
    "tab8 = df[7001:8001]\n",
    "tab9 = df[8001:9001]\n",
    "tab10 = df[9001:10001]\n",
    "tab11 = df[10001:11001]\n",
    "tab12 = df[11001:]\n",
    "All = df\n",
    "\n",
    "tabname1 = tabname + \" 1\"\n",
    "tabname2 = tabname + \" 2\"\n",
    "tabname3 = tabname + \" 3\"\n",
    "tabname4 = tabname + \" 4\"\n",
    "tabname5 = tabname + \" 5\"\n",
    "tabname6 = tabname + \" 6\"\n",
    "tabname7 = tabname + \" 7\"\n",
    "tabname8 = tabname + \" 8\"\n",
    "tabname9 = tabname + \" 9\"\n",
    "tabname10 = tabname + \" 10\"\n",
    "tabname11 = tabname + \" 11\"\n",
    "tabname12 = tabname + \" 12\"\n",
    "tabnameAll = \" All\"\n",
    "\n",
    "with pd.ExcelWriter(HD_Dest) as writer:\n",
    "    tab1[column_order].to_excel(writer, sheet_name=tabname1, index=False)\n",
    "    tab2[column_order].to_excel(writer, sheet_name=tabname2, index=False)\n",
    "    tab3[column_order].to_excel(writer, sheet_name=tabname3, index=False)\n",
    "    tab4[column_order].to_excel(writer, sheet_name=tabname4, index=False)\n",
    "    tab5[column_order].to_excel(writer, sheet_name=tabname5, index=False)\n",
    "    tab6[column_order].to_excel(writer, sheet_name=tabname6, index=False)\n",
    "    tab7[column_order].to_excel(writer, sheet_name=tabname7, index=False)\n",
    "    tab8[column_order].to_excel(writer, sheet_name=tabname8, index=False)\n",
    "    tab9[column_order].to_excel(writer, sheet_name=tabname9, index=False)\n",
    "    tab10[column_order].to_excel(writer, sheet_name=tabname10, index=False)\n",
    "    tab11[column_order].to_excel(writer, sheet_name=tabname11, index=False)\n",
    "    tab12[column_order].to_excel(writer, sheet_name=tabname12, index=False)\n",
    "    All[column_order].to_excel(writer, sheet_name=tabnameAll, index=False)    \n",
    "    \n",
    "suffix = \"_GDrive_All_Folders_1.xlsx\"\n",
    "HD_Dest = Local + only_date + \"_\" + only_time + \"_(\" + dffilecount + \")\" + suffix\n",
    "\n",
    "with pd.ExcelWriter(HD_Dest) as writer:\n",
    "    tab1[column_order].to_excel(writer, sheet_name=tabname1, index=False)\n",
    "\n",
    "suffix = \"_GDrive_All_Folders_2.xlsx\"\n",
    "HD_Dest = Local + only_date + \"_\" + only_time + \"_(\" + dffilecount + \")\" + suffix\n",
    "    \n",
    "with pd.ExcelWriter(HD_Dest) as writer:\n",
    "    tab2[column_order].to_excel(writer, sheet_name=tabname2, index=False)\n",
    "\n",
    "suffix = \"_GDrive_All_Folders_3.xlsx\"\n",
    "HD_Dest = Local + only_date + \"_\" + only_time + \"_(\" + dffilecount + \")\" + suffix\n",
    "    \n",
    "with pd.ExcelWriter(HD_Dest) as writer:\n",
    "    tab3[column_order].to_excel(writer, sheet_name=tabname3, index=False)\n",
    "\n",
    "suffix = \"_GDrive_All_Folders_4.xlsx\"\n",
    "HD_Dest = Local + only_date + \"_\" + only_time + \"_(\" + dffilecount + \")\" + suffix\n",
    "    \n",
    "with pd.ExcelWriter(HD_Dest) as writer:\n",
    "    tab4[column_order].to_excel(writer, sheet_name=tabname4, index=False)\n",
    "\n",
    "suffix = \"_GDrive_All_Folders_5.xlsx\"\n",
    "HD_Dest = Local + only_date + \"_\" + only_time + \"_(\" + dffilecount + \")\" + suffix\n",
    "    \n",
    "with pd.ExcelWriter(HD_Dest) as writer:\n",
    "    tab5[column_order].to_excel(writer, sheet_name=tabname5, index=False)\n",
    "\n",
    "suffix = \"_GDrive_All_Folders_6.xlsx\"\n",
    "HD_Dest = Local + only_date + \"_\" + only_time + \"_(\" + dffilecount + \")\" + suffix\n",
    "    \n",
    "with pd.ExcelWriter(HD_Dest) as writer:\n",
    "    tab6[column_order].to_excel(writer, sheet_name=tabname6, index=False)\n",
    "\n",
    "suffix = \"_GDrive_All_Folders_7.xlsx\"\n",
    "HD_Dest = Local + only_date + \"_\" + only_time + \"_(\" + dffilecount + \")\" + suffix\n",
    "    \n",
    "with pd.ExcelWriter(HD_Dest) as writer:\n",
    "    tab7[column_order].to_excel(writer, sheet_name=tabname7, index=False)\n",
    "\n",
    "suffix = \"_GDrive_All_Folders_8.xlsx\"\n",
    "HD_Dest = Local + only_date + \"_\" + only_time + \"_(\" + dffilecount + \")\" + suffix\n",
    "\n",
    "with pd.ExcelWriter(HD_Dest) as writer:\n",
    "    tab8[column_order].to_excel(writer, sheet_name=tabname8, index=False)\n",
    "    \n",
    "suffix = \"_GDrive_All_Folders_9.xlsx\"\n",
    "HD_Dest = Local + only_date + \"_\" + only_time + \"_(\" + dffilecount + \")\" + suffix\n",
    "    \n",
    "with pd.ExcelWriter(HD_Dest) as writer:\n",
    "    tab9[column_order].to_excel(writer, sheet_name=tabname9, index=False)\n",
    "\n",
    "suffix = \"_GDrive_All_Folders_10.xlsx\"\n",
    "HD_Dest = Local + only_date + \"_\" + only_time + \"_(\" + dffilecount + \")\" + suffix\n",
    "    \n",
    "with pd.ExcelWriter(HD_Dest) as writer:\n",
    "    tab10[column_order].to_excel(writer, sheet_name=tabname10, index=False)\n",
    "\n",
    "suffix = \"_GDrive_All_Folders_11.xlsx\"\n",
    "HD_Dest = Local + only_date + \"_\" + only_time + \"_(\" + dffilecount + \")\" + suffix\n",
    "    \n",
    "with pd.ExcelWriter(HD_Dest) as writer:\n",
    "    tab11[column_order].to_excel(writer, sheet_name=tabname11, index=False)\n",
    "\n",
    "suffix = \"_GDrive_All_Folders_12.xlsx\"\n",
    "HD_Dest = Local + only_date + \"_\" + only_time + \"_(\" + dffilecount + \")\" + suffix\n",
    "\n",
    "with pd.ExcelWriter(HD_Dest) as writer:\n",
    "    tab12[column_order].to_excel(writer, sheet_name=tabname12, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac03460-b175-4596-a5db-606d28a3fd11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
